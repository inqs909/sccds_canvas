{
  "hash": "bc93d11525b57dbd1fbe96c853cb70fb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Statistical Estimators\"\ndate-modified: \"2024-09-28\"\nformat: live-html\nengine: knitr\nwebr:\n  packages:\n    - ggplot2\n---\n\n\n::: {.cell}\n\n:::\n\n\n\n## R Packages Used\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nlibrary(ggplot2)\n```\n:::\n\n\n## Random Sample\n\nWhen collecting a random sample, it is believed that the data being collected comes from a probability distribution denoted as $F(\\boldsymbol \\theta)$, where $\\boldsymbol \\theta = (\\theta_1, \\theta_2, \\ldots, \\theta_p)^{\\mathrm T}$ is a vector or parameters that describe the distribution. It is assumed that the random sample is a collection of random variables, denoted as $X_1, \\cdots, X_n$, that are considered iid (identical and independently distributed[^1]). Using this random sample, one infer the value of the parameters $\\boldsymbol \\theta$ by functions (statistics) on the random sample.\n\n[^1]: This means that the random variables $X_1, \\ldots, X_n$, come from the same distribution and the value for one random variable will not influence the value of a different random variable. See [here](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) for more information.\n\n## Statistical Estimators\n\nA statistical estimator is said to be a function designed to provide a point estimate, or interval estimate, of an unknown parameter in $\\boldsymbol \\theta$. Common statistical estimators can be the mean, $\\bar X = \\frac{1}{n}\\sum^n_{i=1} X_i$, or standard deviation, $S^2 = \\frac{1}{n-1}\\sum^{n}_{i=1}(X_i - \\bar X)^2$. Other estimators can be obtained by applying a procedure such as the [maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation), [method of moments](https://en.wikipedia.org/wiki/Method_of_moments_(statistics)) or a [Bayes Estimator](https://en.wikipedia.org/wiki/Bayes_estimator).\n\n## Sampling Distributions\n\nA sampling distribution can be thought as the distribution of an estimator (statistic). The reason is because the estimator is a function of random variables; therefore, the estimator itself is also a random variable. This means that the estimator will vary based on what was randomly drawn for the sample. For example, $\\bar X = \\frac{1}{n}\\sum^n_{i=1}X_i$ will have a distribution depending on the distribution that generated $X_1, \\ldots, X_n$.\n\n### Normal Distribution Example\n\nAssume that $X_1, \\ldots, X_{25}\\overset{iid}{\\sim}N(8, 3)$, normal distribution with mean 8 and variance 3. Depending on the sample, the value of $\\bar X$ will change due to the randomness being generated. Therefore, a different sample will yield a different value of $\\bar X$. The R code below will demonstrate the potential distribution $\\bar X$ by simulating numerous samples from distribution above and generating the histogram of $\\bar X$.\n\n#### \n\nTo simulate a random sample of 25 that follows a normal distribution, we can use the `rnorm` function. Afterwards, we will compute the mean of the sample.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx1 <- rnorm(25, 8, sqrt(3))\nmean(x1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 8.990763\n```\n\n\n:::\n:::\n\n\nNotice that the value is close to 8. Generate (run the code below) multiple samples and see what mean are values are being produced:\n\n\n::: {.cell}\n```{webr}\nxw <- rnorm(25, 8, sqrt(3))\nmean(xw)\n```\n:::\n\n\nNow to visualize see the distribution of $\\bar X$, we will simulate 10,000 samples, compute the mean of each sample, and construct the a histogram of the computed means.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate 10,000 samples of size 25 \nx_samples <- replicate(10000, rnorm(25, 8, sqrt(3)))\n# Obtain the mean for all the samples\nx_means <- colMeans(x_samples)\n# Plot a histogram of the sample means\ndata.frame(xbar = x_means) |> \n  ggplot(aes(xbar)) +\n  geom_histogram() +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](estimators_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nNotice that the values of $\\bar X$ are bell shaped centered around the value 8. This makes us think that the sampling distribution for $\\bar X$ may follow a normal distribution. In fact, if a random is said to be generated from a normal distribution, then the distribution will also be normally distributed. For this example, the distribution of $\\bar X$ is $N(8, 3/25)$. We can plot the probability density function on the histogram and they will closely align.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plotting the histogram of the sample means\n# And imposing the density function of a normal distribution\n\ndata.frame(xbar = x_means, y = dnorm(x_means, 8, sqrt(3/25))) |> \n  ggplot(aes(xbar, y = after_stat(density))) +\n  geom_histogram() +\n  geom_line(aes(xbar, y), col = \"red\", lwd = 1) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](estimators_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n## Central Limit Theorem\n\nThe central limit theorem is the framework for several of hypothesis tests that are based on probability models.\n\n::: callout-note\n### Central Limit Theorem\n\nIf random variables $X_1, X_2, \\cdots, X_n$ are independent come from the same distribution ($iid$), $E(X_i) = \\mu <\\infty$ (finite), $Var(X_i) = \\sigma^2<\\infty$ (finite), then\n\n$$\n\\frac{\\bar X - \\mu}{\\sigma/\\sqrt n} \\overset{\\circ}{\\sim} N(0,1)\n$$\n\nas $n\\rightarrow \\infty$, which implies:\n\n$$\n\\bar X \\overset{\\circ}{\\sim}  N(\\mu, \\sigma^2/n)\n$$\n:::\n\nThe central limit theorem allows us to assume the distribution of $\\bar X$ regardless of the distribution of the sample $X_1, X_2, \\cdots, X_n$. The only condition is that the expected value and variance exist.\n\n### $\\chi^2$ Example\n\nAssume that $X_1, \\ldots, X_{n}\\overset{iid}{\\sim}\\chi^2(4)$, Chi-Square distribution with 4 degrees of freedom. According to the central limit theorem, as $n\\rightarrow \\infty$, the distribution for $\\bar X$ will approximately be normal with a mean of $4$ and variance $8/n$.\n\n#### Varying Sample Sizes\n\nRun the following examples to show how the distribution begins to follow a normal distribution (red line) as $n$ increases 15, 30, 50, 100, 1000. **Change the number on line 3 to see how the distribtion changes**. \n\n\n\n\n::: {.cell}\n```{webr}\n#| warning: false\n# Generate 1,000 samples of size nn\n# Obtain the mean for all the samples\nnn <- 15\nx_samples <- replicate(1000, rchisq(nn, 4))\nx_means <- colMeans(x_samples)\n\n# Plotting the histogram of the sample means\n# And imposing the density function of a normal distribution\n\ndata.frame(xbar = x_means, y = dnorm(x_means, 4, sqrt(8/nn))) |> \n  ggplot(aes(xbar, y = after_stat(density))) +\n  geom_histogram() +\n  geom_line(aes(xbar, y), col = \"red\", lwd = 1) +\n  ggtitle(paste0(\"N = \", nn)) +\n  theme_bw()\n```\n:::\n\n\n### Poisson Example\n\nAssume that $X_1, \\ldots, X_{n}\\overset{iid}{\\sim}Pois(3.2)$, Poisson  distribution with a rate of 3.2. According to the central limit theorem, as $n\\rightarrow \\infty$, the distribution for $\\bar X$ will approximately be normal with a mean of $3.2$ and variance $3.2/n$.\n\n#### Varying Sample Sizes\n\nRun the following examples to show how the distribution begins to follow a normal distribution (red line) as $n$ increases 15, 30, 50, 100, 1000. **Change the number on line 3 to see how the distribtion changes**. \n\n\n\n\n::: {.cell}\n```{webr}\n#| warning: false\n# Generate 1,000 samples of size nn\n# Obtain the mean for all the samples\nnn <- 15\nx_samples <- replicate(1000, rpois(nn, lambda = 3.2))\nx_means <- colMeans(x_samples)\n\n# Plotting the histogram of the sample means\n# And imposing the density function of a normal distribution\n\ndata.frame(xbar = x_means, y = dnorm(x_means, 3.2, sqrt(3.2/nn))) |> \n  ggplot(aes(xbar, y = after_stat(density))) +\n  geom_histogram() +\n  geom_line(aes(xbar, y), col = \"red\", lwd = 1) +\n  ggtitle(paste0(\"N = \", nn)) +\n  theme_bw()\n```\n:::\n\n\n",
    "supporting": [
      "estimators_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}