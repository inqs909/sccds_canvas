{
  "hash": "b799b432d44fbdb3e1de3e53b5ae716b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"The Bootstrap Method\"\ndate-modified: \"2024-10-14\"\n---\n\n\n\n\n## R Packages Used\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\ntheme_set(theme_bw())\ntheme_update(axis.title = element_text(size = 24))\n```\n:::\n\n\n\nThe Boostrap Method is an approach that can construct the sampling distribution of several estimators, not all estimators, without imposing a mathematical model on the data. The idea is that the sample $X_1, \\ldots, X_n$ is generated from a distribution function called $F(\\theta)$. If the sample is large enough, then the empirical distribution function $\\hat F_n$ should begin to look like the true distribution function $F$. This implies that the sample contains all the information of $F$. Therefore, if we resample from our sample, with replacement, we are then sampling from our empirical distribution $\\hat F_n$, which looks really close to the true distribution function $F$.\n\n## Empirical Distribution Function\n\nThe empirical distribution function is designed to estimate a random variable's distribution function. For an observed sample $\\{x_i\\}^n_{i=1}$, the empirical distribution function is\n\n$$\n\\hat F_n(x) = \\left\\{\\begin{array}{cc}\n0, & x < x_{(1)} \\\\\n\\frac{i}{n},& x_{(i)} \\leq x <x_{(i+1)},\\ i = 1,\\ldots,n-1\\\\\n1,& x_{(n)}\\leq x\n\\end{array}\n\\right.\n$$\n\nwhere $x_{(1)}, \\ldots, x_{(n)}$ is the ordered sample. Looking at the Glivenko-Cantelli Theorem, the empirical distribution function converges to the true function as $n\\rightarrow \\infty$. For a large enough sample, $\\hat F_n$ will contain the same information as $F$.\n\n\n::: callout-note\n### Glivenko-Cantelli Theorem\n\nIf random variables $X_1, X_2, \\cdots, X_n$ are independent come from the same distribution ($iid$), then\n\n$$\n\\hat F_n (x) \\rightarrow F(x)\n$$\n\nconverges uniformly as $n\\rightarrow \\infty$, for more information click [here](https://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_theorem).\n\n:::\n\n\n\nAccordingly, the sample generated has it's own distribution function called $\\hat F_n$ where the probability of seeing any value $x_i$ is $1/n$. Therefore, if we were to resample $\\{x_i\\}^n_{i=1}$, with replacement, it is equivalent to sampling from $\\hat F_n$. \n\n## The Bootstrap Method\n\nThe Bootstrap Method is commonly used to obtain the standard error and or confidence limits of an estimator. By repeatedly sampling from $\\hat F_n$, the sampling distribution of any estimator can be approximated. This is advantageous from standard mathematical models which imposes a distribution on $F$, which may be completely inaccurate. The only assumption being made is that $\\hat F_n$ is close the true distribution function $F$.  \n\n### Algorithm\n\nLet $\\boldsymbol X = (X_1, X_2, \\ldots, X_n)$ be a random sample from a distribution $F$. For an estimator $T(\\cdot)$: \n\n1.  Draw a sample $\\boldsymbol X^*_b$ of size $n$, with replacement, from the original data $\\boldsymbol X$.\n2.  Compute the bootstrap replicate statistic $T*_b = T(\\boldsymbol X^*_b)$, where $T(\\cdot)$ is the function that computes the statistic of interest.\n3.  Repeat steps 1-2 $B$ times to obtain $B$ bootstrap replicates $T^*{T*_1, T*_2, ..., T*_B}$.\n\n\nThe computed statistics from $B$ samples are the empirical bootstrap distribution of the estimator, $T(\\boldsymbol X)$. This can be used to compute the standard error, bias, and confidence interval of the estimator $T(\\boldsymbol X)$. The number or replicates needed is open to discussion; however, research has shown $B=200$ to suffiece. Larger $B$ may be needed for confidence limit estimation. Another rule of thumb is having $B=n$; however, this may be unfeasible for extremely large samples or computationally heavy tasks.\n\n### Standard Error Estimation\n\nThe bootstrap-based standard error of an estimator is shown to provide an unbiased estimate of the true standard error. We can compute the standard eror using the following formula:\n\n$$\n\\hat{se}\\left\\{T(\\boldsymbol X)\\right\\} = \\sqrt{\\frac{1}{B-1}\\sum^B_{i=1}(T^*_i-\\bar T^*)^2}\n$$\nwhere $\\bar T^* = \\frac{1}{B}\\sum^B_{i=1}T^*_i$.\n\n\n### Bias Estimation\n\n### Confidence Limits Estimation\n\n\n\n### Limitation to Boostrap Methods\n\nLet's say we randomly sample 5 data points from a Poisson Distribution with a rate of 5^[Note: The `set.seed` function ensures that the same random sample will be generated.]:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nx <- rpois(5, 1.5)\nx\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 3 4 1 3 2\n```\n\n\n:::\n:::\n\n\n\nIf we were to use this sample and generate bootstrap estimates, we will obtain inaccurate results. This is because the empirical distribution function is a poor estimate of the true distribution function. One reason being that the $P(X = 0) = 0.2231$, and our sample does not have any 0 values. Any bootstrap samples produced will never carry that information. This is why a large sample is needed so the sample space can be thoroughly explored.\n\n## Examples\n\nThe examples below illustrate how to compute the bootstrap standard errors in different scenarios.\n\n### Poisson Distribution\n\n### Linear Regression\n\n### Logistic Regression\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}