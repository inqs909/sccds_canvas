---
title: "The Bootstrap Method"
date-modified: "`r Sys.Date()`"
---


## R Packages Used

```{r}
#| code-fold: show

library(tidyverse)
library(palmerpenguins)

theme_set(theme_bw())
theme_update(axis.title = element_text(size = 24))



```

The Boostrap Method is an approach that can construct the sampling distribution of several estimators, not all estimators, without imposing a mathematical model on the data. The idea is that the sample $X_1, \ldots, X_n$ is generated from a distribution function called $F(\theta)$. If the sample is large enough, then the empirical distribution function $\hat F_n$ should begin to look like the true distribution function $F$. This implies that the sample contains all the information of $F$. Therefore, if we resample from our sample, with replacement, we are then sampling from our empirical distribution $\hat F_n$, which looks really close to the true distribution function $F$.

## Empirical Distribution Function

The empirical distribution function is designed to estimate a random variable's distribution function. For an observed sample $\{x_i\}^n_{i=1}$, the empirical distribution function is

$$
\hat F_n(x) = \left\{\begin{array}{cc}
0, & x < x_{(1)} \\
\frac{i}{n},& x_{(i)} \leq x <x_{(i+1)},\ i = 1,\ldots,n-1\\
1,& x_{(n)}\leq x
\end{array}
\right.
$$

where $x_{(1)}, \ldots, x_{(n)}$ is the ordered sample. Looking at the Glivenko-Cantelli Theorem, the empirical distribution function converges to the true function as $n\rightarrow \infty$. For a large enough sample, $\hat F_n$ will contain the same information as $F$.


::: callout-note
### Glivenko-Cantelli Theorem

If random variables $X_1, X_2, \cdots, X_n$ are independent come from the same distribution ($iid$), then

$$
\hat F_n (x) \rightarrow F(x)
$$

converges uniformly as $n\rightarrow \infty$, for more information click [here](https://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_theorem).

:::



Accordingly, the sample generated has it's own distribution function called $\hat F_n$ where the probability of seeing any value $x_i$ is $1/n$. Therefore, if we were to resample $\{x_i\}^n_{i=1}$, with replacement, it is equivalent to sampling from $\hat F_n$. 

## The Bootstrap Method

The Bootstrap Method is commonly used to obtain the standard error and or confidence limits of an estimator. By repeatedly sampling from $\hat F_n$, the sampling distribution of any estimator can be approximated. This is advantageous from standard mathematical models which imposes a distribution on $F$, which may be completely inaccurate. The only assumption being made is that $\hat F_n$ is close the true distribution function $F$.  

### Algorithm

Let $\boldsymbol X = (X_1, X_2, \ldots, X_n)$ be a random sample from a distribution $F$. For an estimator $T(\cdot)$: 

1.  Draw a sample $\boldsymbol X^*_b$ of size $n$, with replacement, from the original data $\boldsymbol X$.
2.  Compute the bootstrap replicate statistic $T*_b = T(\boldsymbol X^*_b)$, where $T(\cdot)$ is the function that computes the statistic of interest.
3.  Repeat steps 1-2 $B$ times to obtain $B$ bootstrap replicates $T^*{T*_1, T*_2, ..., T*_B}$.


The computed statistics from $B$ samples are the empirical bootstrap distribution of the estimator, $T(\boldsymbol X)$. This can be used to compute the standard error, bias, and confidence interval of the estimator $T(\boldsymbol X)$. The number or replicates needed is open to discussion; however, research has shown $B=200$ to suffiece. Larger $B$ may be needed for confidence limit estimation. Another rule of thumb is having $B=n$; however, this may be unfeasible for extremely large samples or computationally heavy tasks.

### Standard Error Estimation

The bootstrap-based standard error of an estimator is shown to provide an unbiased estimate of the true standard error. We can compute the standard eror using the following formula:

$$
\hat{se}\left\{T(\boldsymbol X)\right\} = \sqrt{\frac{1}{B-1}\sum^B_{i=1}(T^*_i-\bar T^*)^2}
$$
where $\bar T^* = \frac{1}{B}\sum^B_{i=1}T^*_i$.


### Bias Estimation

### Confidence Limits Estimation



### Limitation to Boostrap Methods

Let's say we randomly sample 5 data points from a Poisson Distribution with a rate of 5^[Note: The `set.seed` function ensures that the same random sample will be generated.]:

```{r}
set.seed(42)
x <- rpois(5, 1.5)
x
```

If we were to use this sample and generate bootstrap estimates, we will obtain inaccurate results. This is because the empirical distribution function is a poor estimate of the true distribution function. One reason being that the $P(X = 0) = 0.2231$, and our sample does not have any 0 values. Any bootstrap samples produced will never carry that information. This is why a large sample is needed so the sample space can be thoroughly explored.

## Examples

The examples below illustrate how to compute the bootstrap standard errors in different scenarios.

### Poisson Distribution

### Linear Regression

### Logistic Regression


