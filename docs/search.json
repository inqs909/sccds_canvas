[
  {
    "objectID": "randomizations.html",
    "href": "randomizations.html",
    "title": "Randomization (Permutations) Tests",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(palmerpenguins)\n\ntheme_set(theme_bw())\ntheme_update(axis.title = element_text(size = 24))\n\n\nshuffle &lt;- function(x){\n  n &lt;- length(x)\n  return(sample(x, n))\n}\n\npenguins &lt;- penguins |&gt; drop_na()"
  },
  {
    "objectID": "randomizations.html#r-packages-and-functions-used",
    "href": "randomizations.html#r-packages-and-functions-used",
    "title": "Randomization (Permutations) Tests",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(palmerpenguins)\n\ntheme_set(theme_bw())\ntheme_update(axis.title = element_text(size = 24))\n\n\nshuffle &lt;- function(x){\n  n &lt;- length(x)\n  return(sample(x, n))\n}\n\npenguins &lt;- penguins |&gt; drop_na()"
  },
  {
    "objectID": "randomizations.html#permutation-tests",
    "href": "randomizations.html#permutation-tests",
    "title": "Randomization (Permutations) Tests",
    "section": "Permutation Tests",
    "text": "Permutation Tests\nPermutation tests conducts a statistical test by constructing the null distribution by rearranging the data points in a sample.\nNull hypothesis states that the rearrangements of the data points are random.\nAlternative hypothesis states that the rearrangement of the data points aren’t random."
  },
  {
    "objectID": "randomizations.html#permutation-distributions",
    "href": "randomizations.html#permutation-distributions",
    "title": "Randomization (Permutations) Tests",
    "section": "Permutation Distributions",
    "text": "Permutation Distributions\n\n\n\nNull\n\\[\nF_x = F_y\n\\]\n\n\n\nAlternative\n\\[\nF_x \\neq F_y\n\\]\n\n\n\nSuppose \\(\\{X_i, Y_i\\}^n_{i=1}\\) is an observed permutation, \\(X = \\{X_1, \\ldots, X_n\\}\\), \\(Y = \\{Y_1, \\ldots, Y_n\\}\\).\nThe Probability of any permuation is \\(1/n!\\).\nTherefore, for a statistic \\(T(X,Y)\\), a sampling distribution can be constructed by all the different permutations.\nA hypothesis test can be conducted by observing the proportion of more extreme values of the sample statistic."
  },
  {
    "objectID": "randomizations.html#approximate-permutation-distribution",
    "href": "randomizations.html#approximate-permutation-distribution",
    "title": "Randomization (Permutations) Tests",
    "section": "Approximate Permutation Distribution",
    "text": "Approximate Permutation Distribution\nConstructing the distribution for the permutations can be challenging if the number of permutations is high! If \\(n=100\\), the number of permutations is \\(100!\\):\n\nfactorial(100)\n\n[1] 9.332622e+157\n\n\nTherefore, simulation techniques are needed to approximate the p-value.\nBy randomly drawing from the sample, we can approximate the p-value.\n\nAlgorithm\n\nConstruct a new data set\nFix the predictor (\\(X\\)) variable and randomly assign a data point \\(Y\\) to the fixed \\(X\\)\nCompute a test statistic using the new data set and store the value\nRepeat steps 1 and 2 for \\(N\\) times\nCompute the test statistic from the empirical sample (un-permutated)\nCount how many permutated statistics that are more extreme than the sample test statistic (\\(m\\))\nCompute the Monte Carlo p-value\n\n\\[\np = \\frac{m +1}{N + 1}\n\\]"
  },
  {
    "objectID": "randomizations.html#example-emperical-data",
    "href": "randomizations.html#example-emperical-data",
    "title": "Randomization (Permutations) Tests",
    "section": "Example: Emperical Data",
    "text": "Example: Emperical Data\n\n\nCode\npenguins |&gt; ggplot(aes(x=species, y = body_mass_g)) +\n  geom_boxplot() +\n  geom_jitter() +\n  labs(x = \"Species\", y = \"Body Mass\")\n\n\n\n\n\n\n\n\n\n\nExample: Random Shuffling\n\n\nCode\npenguins |&gt; ggplot() +\n  labs(x = \"Species\", y = \"Body Mass\") + \n  geom_jitter(aes(species, shuffle(body_mass_g)))\n\n\n\n\n\n\n\n\n\n\n\nExample: Random/Emperical\n\n\nCode\npenguins |&gt; ggplot(aes(x = species, y = body_mass_g)) +\n  labs(x = \"Species\", y = \"Body Mass\") + \n  geom_jitter(col = \"red\") +\n  geom_jitter(aes(species, shuffle(body_mass_g)))\n\n\n\n\n\n\n\n\n\n\n\nExample: Random/Emperical\n\n\nCode\npenguins |&gt; ggplot(aes(x = species, y = body_mass_g)) +\n  labs(x = \"Species\", y = \"Body Mass\") + \n  geom_jitter(aes(species, shuffle(body_mass_g))) +\n  geom_jitter(aes(species, shuffle(body_mass_g))) +\n  geom_jitter(aes(species, shuffle(body_mass_g))) +\n  geom_jitter(aes(species, shuffle(body_mass_g))) +\n  geom_jitter(aes(species, shuffle(body_mass_g))) +\n  geom_jitter(aes(species, shuffle(body_mass_g))) +\n  geom_jitter(aes(species, shuffle(body_mass_g))) +\n  geom_jitter(aes(species, shuffle(body_mass_g))) +\n  geom_jitter(aes(species, shuffle(body_mass_g))) +\n  geom_jitter(aes(species, shuffle(body_mass_g))) +\n  geom_jitter(col = \"red\") \n\n\n\n\n\n\n\n\n\n\n\nANOVA\nWe want to determine if body mass of penguins are different for different species.\n\npenguins |&gt; aov(body_mass_g ~ species, data = _) |&gt; anova()\n\nAnalysis of Variance Table\n\nResponse: body_mass_g\n           Df    Sum Sq  Mean Sq F value    Pr(&gt;F)    \nspecies     2 145190219 72595110  341.89 &lt; 2.2e-16 ***\nResiduals 330  70069447   212332                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nPermutation Test\n\nf_stat &lt;- penguins |&gt; \n  aov(body_mass_g ~ species, data = _) |&gt; \n  anova() |&gt; \n  _$`F value`[1]\n  \n\nf_sim &lt;- function(i){\n  ff &lt;- penguins |&gt; \n    aov(shuffle(body_mass_g) ~ species, data = _) |&gt; \n    anova() |&gt; \n    _$`F value`[1]\n  return(ff)\n}\n\nf_dist &lt;- replicate(10000, f_sim(1))\n\ntibble(x= f_dist) |&gt; \n  ggplot(aes(x, y = ..density..)) +\n  geom_histogram() +\n  geom_vline(xintercept = f_stat)\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nsum(f_stat &lt; f_dist) + 1 / (length(f_dist) + 1)\n\n[1] 9.999e-05"
  },
  {
    "objectID": "randomizations.html#permutation-example",
    "href": "randomizations.html#permutation-example",
    "title": "Randomization (Permutations) Tests",
    "section": "Permutation Example",
    "text": "Permutation Example\n\nPermutation Linear Regression\n\nKeep the predictor values fixed (unchanged)\nRandomly assign the sampled outcome values to a fixed predictor\nCompute the regression coefficients for the predictor variable\n\n\n\nSimulated Permutation"
  },
  {
    "objectID": "randomizations.html#permutations",
    "href": "randomizations.html#permutations",
    "title": "Randomization (Permutations) Tests",
    "section": "Permutations",
    "text": "Permutations"
  },
  {
    "objectID": "randomizations.html#permutation-test-1",
    "href": "randomizations.html#permutation-test-1",
    "title": "Randomization (Permutations) Tests",
    "section": "Permutation Test",
    "text": "Permutation Test"
  },
  {
    "objectID": "estimators.html",
    "href": "estimators.html",
    "title": "Statistical Estimators",
    "section": "",
    "text": "library(ggplot2)"
  },
  {
    "objectID": "estimators.html#r-packages-loaded",
    "href": "estimators.html#r-packages-loaded",
    "title": "Statistical Estimators",
    "section": "",
    "text": "library(ggplot2)"
  },
  {
    "objectID": "estimators.html#random-sample",
    "href": "estimators.html#random-sample",
    "title": "Statistical Estimators",
    "section": "Random Sample",
    "text": "Random Sample\nWhen collecting a random sample, it is believed that the data being collected comes from a probability distribution denoted as \\(F(\\boldsymbol \\theta)\\), where \\(\\boldsymbol \\theta = (\\theta_1, \\theta_2, \\ldots, \\theta_p)^{\\mathrm T}\\) is a vector or parameters that describe the distribution. It is assumed that the random sample is a collection of random variables, denoted as \\(X_1, \\cdots, X_n\\), that are considered iid (identical and independently distributed1). Using this random sample, one infer the value of the parameters \\(\\boldsymbol \\theta\\) by functions (statistics) on the random sample."
  },
  {
    "objectID": "estimators.html#statistical-estimators",
    "href": "estimators.html#statistical-estimators",
    "title": "Statistical Estimators",
    "section": "Statistical Estimators",
    "text": "Statistical Estimators\nA statistical estimator is said to be a function designed to provide a point estimate, or interval estimate, of an unknown parameter in \\(\\boldsymbol \\theta\\). Common statistical estimators can be the mean, \\(\\bar X = \\frac{1}{n}\\sum^n_{i=1} X_i\\), or standard deviation, \\(S^2 = \\frac{1}{n-1}\\sum^{n}_{i=1}(X_i - \\bar X)^2\\). Other estimators can be obtained by applying a procedure such as the maximum likelihood estimation, method of moments or a Bayes Estimator."
  },
  {
    "objectID": "estimators.html#sampling-distributions",
    "href": "estimators.html#sampling-distributions",
    "title": "Statistical Estimators",
    "section": "Sampling Distributions",
    "text": "Sampling Distributions\nA sampling distribution can be thought as the distribution of an estimator (statistic). The reason is because the estimator is a function of random variables; therefore, the estimator itself is also a random variable. This means that the estimator will vary based on what was randomly drawn for the sample. For example, \\(\\bar X = \\frac{1}{n}\\sum^n_{i=1}X_i\\) will have a distribution depending on the distribution that generated \\(X_1, \\ldots, X_n\\).\n\nNormal Distribution Example\nAssume that \\(X_1, \\ldots, X_{25}\\overset{iid}{\\sim}N(8, 3)\\), normal distribution with mean 8 and variance 3. Depending on the sample, the value of \\(\\bar X\\) will change due to the randomness being generated. Therefore, a different sample will yield a different value of \\(\\bar X\\). The R code below will demonstrate the potential distribution \\(\\bar X\\) by simulating numerous samples from distribution above and generating the histogram of \\(\\bar X\\).\n\n\nTo simulate a random sample of 25 that follows a normal distribution, we can use the rnorm function. Afterwards, we will compute the mean of the sample.\n\nx1 &lt;- rnorm(25, 8, sqrt(3))\nmean(x1)\n\n[1] 8.151327\n\n\nNotice that the value is close to 8. If we generate two different samples, notice how all means calculated are different from each other.\n\nx2 &lt;- rnorm(25, 8, sqrt(3))\nmean(x2)\n\n[1] 8.054311\n\n\n\nx3 &lt;- rnorm(25, 8, sqrt(3))\nmean(x3)\n\n[1] 7.446017\n\n\nNow to visualize see the distribution of \\(\\bar X\\), we will simulate 10,000 samples, compute the mean of each sample, and construct the a histogram of the computed means.\n\n# Generate 10,000 samples of size 25 \nx_samples &lt;- replicate(10000, rnorm(25, 8, sqrt(3)))\n# Obtain the mean for all the samples\nx_means &lt;- colMeans(x_samples)\n# Plot a histogram of the sample means\ndata.frame(xbar = x_means) |&gt; \n  ggplot(aes(xbar)) +\n  geom_histogram() +\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nNotice that the values of \\(\\bar X\\) are bell shaped centered around the value 8. This makes us think that the sampling distribution for \\(\\bar X\\) may follow a normal distribution. In fact, if a random is said to be generated from a normal distribution, then the distribution will also be normally distributed. For this example, the distribution of \\(\\bar X\\) is \\(N(8, 3/25)\\). We can plot the probability density function on the histogram and they will closely align.\n\n# Plotting the histogram of the sample means\n# And imposing the density function of a normal distribution\n\ndata.frame(xbar = x_means, y = dnorm(x_means, 8, sqrt(3/25))) |&gt; \n  ggplot(aes(xbar, y = after_stat(density))) +\n  geom_histogram() +\n  geom_line(aes(xbar, y), col = \"red\", lwd = 1) +\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "estimators.html#central-limit-theorem",
    "href": "estimators.html#central-limit-theorem",
    "title": "Statistical Estimators",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nThe central limit theorem is the framework for several of hypothesis tests that are based on probability models.\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\nIf random variables \\(X_1, X_2, \\cdots, X_n\\) are independent come from the same distribution (\\(iid\\)), \\(E(X_i) = \\mu &lt;\\infty\\) (finite), \\(Var(X_i) = \\sigma^2&lt;\\infty\\) (finite), then\n\\[\n\\frac{\\bar X - \\mu}{\\sigma/\\sqrt n} \\overset{\\circ}{\\sim} N(0,1)\n\\]\nas \\(n\\rightarrow \\infty\\), which implies:\n\\[\n\\bar X \\overset{\\circ}{\\sim}  N(\\mu, \\sigma^2/n)\n\\]\n\n\nThe central limit theorem allows us to assume the distribution of \\(\\bar X\\) regardless of the distribution of the sample \\(X_1, X_2, \\cdots, X_n\\). The only condition is that the expected value and variance exist.\n\n\\(\\chi^2\\) Example\nAssume that \\(X_1, \\ldots, X_{25}\\overset{iid}{\\sim}\\chi^2(4)\\), Chi-Square distribution with 4 degrees of freedom. According to the central limit theorem, as \\(n\\rightarrow \\infty\\), the distribution for \\(\\bar X\\) will approximately be normal with a mean of \\(4\\) and variance \\(8/n\\). The following examples show how the distribution begin to follow a normal distribution (red line) as \\(n\\) increases 15, 30, 50, 100, 1000.\n\n\\(n = 15\\)\n\n# Generate 10,000 samples of size 15 \n# Obtain the mean for all the samples\nx_samples &lt;- replicate(10000, rchisq(15, 4))\nx_means &lt;- colMeans(x_samples)\n\n# Plotting the histogram of the sample means\n# And imposing the density function of a normal distribution\ndata.frame(xbar = x_means, y = dnorm(x_means, 4, sqrt(8/15))) |&gt; \n  ggplot(aes(xbar, y = after_stat(density))) +\n  geom_histogram() +\n  geom_line(aes(xbar, y), col = \"red\", lwd = 1) +\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\\(n = 30\\)\n\n# Generate 10,000 samples of size 30\n# Obtain the mean for all the samples\nx_samples &lt;- replicate(10000, rchisq(30, 4))\nx_means &lt;- colMeans(x_samples)\n\n# Plotting the histogram of the sample means\n# And imposing the density function of a normal distribution\n\ndata.frame(xbar = x_means, y = dnorm(x_means, 4, sqrt(8/30))) |&gt; \n  ggplot(aes(xbar, y = after_stat(density))) +\n  geom_histogram() +\n  geom_line(aes(xbar, y), col = \"red\", lwd = 1) +\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\\(n = 50\\)\n\n# Generate 10,000 samples of size 50\n# Obtain the mean for all the samples\nx_samples &lt;- replicate(10000, rchisq(50, 4))\nx_means &lt;- colMeans(x_samples)\n\n# Plotting the histogram of the sample means\n# And imposing the density function of a normal distribution\ndata.frame(xbar = x_means, y = dnorm(x_means, 4, sqrt(8/50))) |&gt; \n  ggplot(aes(xbar, y = after_stat(density))) +\n  geom_histogram() +\n  geom_line(aes(xbar, y), col = \"red\", lwd = 1) +\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\\(n = 100\\)\n\n# Generate 10,000 samples of size 100\n# Obtain the mean for all the samples\nx_samples &lt;- replicate(10000, rchisq(100, 4))\nx_means &lt;- colMeans(x_samples)\n\n# Plotting the histogram of the sample means\n# And imposing the density function of a normal distribution\ndata.frame(xbar = x_means, y = dnorm(x_means, 4, sqrt(8/100))) |&gt; \n  ggplot(aes(xbar, y = after_stat(density))) +\n  geom_histogram() +\n  geom_line(aes(xbar, y), col = \"red\", lwd = 1) +\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\\(n = 1000\\)\n\n# Generate 10,000 samples of size 1000 \n# Obtain the mean for all the samples\nx_samples &lt;- replicate(10000, rchisq(1000, 4))\nx_means &lt;- colMeans(x_samples)\n\n# Plotting the histogram of the sample means\n# And imposing the density function of a normal distribution\ndata.frame(xbar = x_means, y = dnorm(x_means, 4, sqrt(8/1000))) |&gt; \n  ggplot(aes(xbar, y = after_stat(density))) +\n  geom_histogram() +\n  geom_line(aes(xbar, y), col = \"red\", lwd = 1) +\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "estimators.html#footnotes",
    "href": "estimators.html#footnotes",
    "title": "Statistical Estimators",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis means that the random variables \\(X_1, \\ldots, X_n\\), come from the same distribution and the value for one random variable will not influence the value of a different random variable. See here for more information.↩︎"
  },
  {
    "objectID": "bootstrapping.html",
    "href": "bootstrapping.html",
    "title": "The Bootstrap Method",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(palmerpenguins)\n\ntheme_set(theme_bw())\ntheme_update(axis.title = element_text(size = 24))\n\nshuffle &lt;- function(x){\n  n &lt;- length(x)\n  return(sample(x, n))\n}\n\npenguins &lt;- penguins |&gt; drop_na()\nLet \\(X_1, \\ldots, X_n\\) come from a distribution \\(F(\\theta)\\) and let \\(T\\) be an estimator for \\(\\theta\\)\nThe Boostrap Method is an approach that can construct the sampling distribution of several estimators, not all estimators, without imposing a mathematical model on the data. The idea is that the sample \\(X_1, \\ldots, X_n\\) is generated from a distribution function called \\(F(\\theta)\\). If the sample is large enough, then the empirical distribution function \\(\\hat F\\) should begin to look like the true distribution function \\(F\\) so long as the sample is large enough. This implies the sample can be assumed to contain all the information of \\(F\\). Therefore, if we resample from our sample, with replacement, we are then sampling from our empirical distribution \\(\\hat F\\), which looks really close to the true \\(F\\).\nThe new resampled data, \\(B_1\\), is similar to taking a random sample from \\(F\\). The resample \\(B\\) will produce a value for an estimator, called \\(T_1\\). Since the estimator can change values due to the values of the"
  },
  {
    "objectID": "bootstrapping.html#empirical-distribution-function",
    "href": "bootstrapping.html#empirical-distribution-function",
    "title": "The Bootstrap Method",
    "section": "Empirical Distribution Function",
    "text": "Empirical Distribution Function\nThe empirical distribution function is designed to estimate a random variable’s distribution function. For an observed sample \\(\\{x_i\\}^n_{i=1}\\), the empirical distribution function is\n\\[\nF_n(x) \\left\\{\\begin{array}{cc}\n0, & x &lt; x_{(1)} \\\\\n\\frac{i}{n},& x_{(i)} \\leq x &lt;x_{(i+1)},\\ i = 1,\\ldots,n-1\\\\\n1,& x_{(n)}\\leq x\n\\end{array}\n\\right.\n\\]\nwhere \\(x_{(1)}, \\ldots, x_{(n)}\\) are the ordered sample.\n\nSampling an unknown \\(F\\)\nThe idea behind bootstrapping is that the data comes from a distribution \\(F\\) with unknown parameters.\nUsing the sample, we can get parameters that explain a parameteric distribution or the emperical distribution for a nonparameteric approach."
  },
  {
    "objectID": "bootstrapping.html#the-bootstrap-method",
    "href": "bootstrapping.html#the-bootstrap-method",
    "title": "The Bootstrap Method",
    "section": "The Bootstrap Method",
    "text": "The Bootstrap Method\nThe Bootstrap Method utilizes the sample to describe the target distribution function to construct a sampling mechanism of the target distribution.\nThis method will allow us to construct a new sample that targets the distribution.\nWe can then construct the sampling distribution of a statistic based on the data.\n\nStandard Error\nThe bootstrap-based standard error of a test statistic is shown to provide an unbiased estimate of the true standard error.\n\n\nLimitation to Boostrap Methods\nThe assumption is that the data provides a good estimate of the distribution function.\nIf the data set is small, it may not contain enough information to accurately describe the distribution."
  },
  {
    "objectID": "bootstrapping.html#parameteric-bootstrap",
    "href": "bootstrapping.html#parameteric-bootstrap",
    "title": "The Bootstrap Method",
    "section": "Parameteric Bootstrap",
    "text": "Parameteric Bootstrap\nParametric bootstrap methods are statistical techniques used to estimate the sampling distribution of an estimator or test statistic by resampling with a model-based approach. This method assumes that the data follow a known probability distribution, and utilizes the estimated statistics as the parameters for the distribution function to construct the sampling distribution.\n\nParameteric Bootstrap Algorithm\n\nEstimate the Parameters: Fit a parametric model to the observed data and estimate the parameters of the model.\nGenerate Bootstrap Samples: Using the estimated parameters, generate a large number of new data sets (bootstrap samples) from the fitted model. These samples are simulated data sets that mimic the original data but are generated from the parametric model.\nCompute the Statistic of Interest: For each bootstrap sample, calculate the statistic of interest (e.g., the mean, variance, regression coefficients, etc.).\nConstruct the Sampling Distribution: Use the calculated statistics from all the bootstrap samples to construct an empirical sampling distribution.\nEstimate Confidence Intervals: Use the empirical sampling distribution to estimate confidence intervals.\n\n\n\nExample\nUse a parameteric bootstrap model to determine the standard errors of the mean body mass of each penguin species.\n\npenguins |&gt; group_by(species) |&gt; \n  summarise(mean = mean(body_mass_g),\n            se = sd(body_mass_g) / sqrt(n()))\n\n# A tibble: 3 × 3\n  species    mean    se\n  &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie    3706.  38.0\n2 Chinstrap 3733.  46.6\n3 Gentoo    5092.  46.0\n\n\nAnswer:\n\nmeans &lt;- penguins$body_mass_g |&gt; tapply(penguins$species, mean)\nnns &lt;- penguins$body_mass_g |&gt; tapply(penguins$species, length)\nsds &lt;- penguins$body_mass_g |&gt; tapply(penguins$species, sd)\nAmeans &lt;- numeric(10000)\nCmeans &lt;- numeric(10000)\nGmeans &lt;- numeric(10000)\nfor (i in 1:10000){\n  Ameans[i] &lt;- rnorm(nns[1], mean = means[1], sd = sds[1]) |&gt; mean()\n  Cmeans[i] &lt;- rnorm(nns[2], mean = means[2], sd = sds[2]) |&gt; mean()\n  Gmeans[i] &lt;- rnorm(nns[3], mean = means[3], sd = sds[3]) |&gt; mean()\n}"
  },
  {
    "objectID": "bootstrapping.html#nonparameteric-bootsrap",
    "href": "bootstrapping.html#nonparameteric-bootsrap",
    "title": "The Bootstrap Method",
    "section": "Nonparameteric Bootsrap",
    "text": "Nonparameteric Bootsrap\nThe nonparameteric approach assumes that distribution function of the data does not follow a common distribution function. Therefore, the data itself will be contain all the information needed to construct the sampling distribution.\nThis requires sampling with replacement.\n\nNonparameteric Bootstrap Algorithm\n\nDraw a sample \\(X*\\) of size \\(n\\) with replacement from the original data \\(X\\).\n\n\\(n\\) is the size of the data\n\nCompute the bootstrap replicate statistic \\(T* = g(X*)\\), where \\(g(\\cdot)\\) is the function that computes the statistic of interest.\nRepeat steps 1-2 \\(B\\) times to obtain \\(B\\) bootstrap replicates \\({T*_1, T*_2, ..., T*_B}\\).\nThe computed statistics from \\(B\\) samples are the empirical bootstrap distribution of the statistic, \\(g(X)\\).\nCalculate the bootstrap standard error of the statistic, \\(se_b(g(X))\\), as the standard deviation of the bootstrap replicates.\nCalculate the bootstrap confidence interval for the statistic, \\(CI(g(X))\\), with the \\(\\alpha\\) and \\((1-\\alpha)%\\) percentiles of the bootstrap replicates, where \\(\\alpha\\) is the desired level of significance.\n\n\n\nExample\nFitting the following model:\n\nlibrary(palmerpenguins)\nlibrary(tidyverse)\npenguins &lt;- penguins |&gt; drop_na()\npenguins |&gt; lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm,\n               data = _)\n\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm + bill_length_mm + \n    bill_depth_mm, data = penguins)\n\nCoefficients:\n      (Intercept)  flipper_length_mm     bill_length_mm      bill_depth_mm  \n        -6445.476             50.762              3.293             17.836  \n\n\nObtain the Bootstrap-based Standard Errors for the regression coefficients. Use \\(B=1000\\) bootstrap samples."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SCCDS PAGES",
    "section": "",
    "text": "This website contains web pages housed for Canvas use."
  },
  {
    "objectID": "mc.html",
    "href": "mc.html",
    "title": "Monte Carlo Methods",
    "section": "",
    "text": "library(tidyverse)\nlibrary(patchwork)\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "mc.html#r-setup",
    "href": "mc.html#r-setup",
    "title": "Monte Carlo Methods",
    "section": "",
    "text": "library(tidyverse)\nlibrary(patchwork)\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "mc.html#distributions-in-r",
    "href": "mc.html#distributions-in-r",
    "title": "Monte Carlo Methods",
    "section": "Distributions in R",
    "text": "Distributions in R\nSeveral common distributions can be utilized in R with the 4 common functions:\n\n\n\n\n\n\n\nLetter\nFunctionality\n\n\n\n\nd\nreturns the height of the probability density/mass function\n\n\np\nreturns the cumulative density function value\n\n\nq\nreturns the inverse cumulative density function (percentiles)\n\n\nr\nreturns a randomly generated number"
  },
  {
    "objectID": "mc.html#random-number-generator",
    "href": "mc.html#random-number-generator",
    "title": "Monte Carlo Methods",
    "section": "Random Number Generator",
    "text": "Random Number Generator"
  },
  {
    "objectID": "mc.html#generating-random-numbers",
    "href": "mc.html#generating-random-numbers",
    "title": "Monte Carlo Methods",
    "section": "Generating Random Numbers",
    "text": "Generating Random Numbers\nA number is an outcome from a random experiment.\nRandom experiment is an experiment where the outcome is not predicted. The outcomes have a probability of being observed, whether equal or not.\n\nPsuedo Random Numbers\nThese methods are considered time-consuming when a large number values are necessary.\nWith the advent of computers, random number can be generated with the use deterministic algorithms, where a mechanism is used to make it random, such as time. Computer-generated random numbers are considered psuedo random numbers because an algorithm is used to generate them given an initial single value, known as a seed.\nSupplying a seed to a random number generator will ensure that the same numbers are produced every time.\n\n\nMersenne Twister\nThe Mersenne Twister is a widely used pseudorandom number generator (PRNG) known for its high quality and efficiency. It was developed by Makoto Matsumoto and Takuji Nishimura in 1997.\nThe default random number generator in R."
  },
  {
    "objectID": "mc.html#uniform-distribution-r",
    "href": "mc.html#uniform-distribution-r",
    "title": "Monte Carlo Methods",
    "section": "Uniform Distribution R",
    "text": "Uniform Distribution R\n\nDescriptionCode\n\n\nThe runif function in R will generate a value the come from a uniform distribution.\nrunif arguments:\n\nn: number of values to generate\nmin: the smallest possible value to generate\nmax: the largest possible value to generate\n\n\n\n\n\nCode\nrunif(1, 0, 1)\n\n\n[1] 0.7273112"
  },
  {
    "objectID": "mc.html#random-variable-generations",
    "href": "mc.html#random-variable-generations",
    "title": "Monte Carlo Methods",
    "section": "Random Variable Generations",
    "text": "Random Variable Generations\n\nRandom Variable Generation\nSeveral distribution, common and uncommon, can be generated using a uniform random variables.\nMore complex distributions may require the use of common distributions.\n\n\nInverse-Transform Method\n\na &lt;- -20\nb &lt;- 4\nx &lt;- seq(a, b, length.out = 1000)\npnorm(x, -8, sqrt(10)) |&gt; tibble(x = x, y = _) |&gt; \nggplot(aes(x,y)) +\ngeom_line() +\ntheme_bw() +\nggtitle(\"CDF\") +\nylab(paste0(\"P(X\",\"\\u2264\",\" x)\"))\n\n\n\n\n\n\n\n\n\n\nInverse-Transformation Algorithm\n\nGenerate a random value \\(U\\) that follows a \\(U(0,1)\\)\nUsing the CDF (\\(F(X)\\)) for random variable \\(X\\), compute:\n\n\\[\nX = F^{-1}(U)\n\\]\n\n\nExponential Distribution\nAn exponential random variable is characterized by the exponential distribution, used to model waiting times or the time until an event occurs a certain number of times.\nThe exponential distribution is a gamma random variable with \\(\\alpha = 1\\).\n\n\nExponential Distribution\n\\[\nf(x) = \\frac{1}{\\lambda} \\exp\\left\\{-\\frac{x}{\\lambda}\\right\\}\n\\]\n\\[\nF(x) = 1-\\exp\\left\\{-\\frac{x}{\\lambda}\\right\\}\n\\]\n\\[\nF^{-1}(x) = -\\lambda \\log(1-x)\n\\]\n\n\nSimulating an Exponential RV\n\\[\nX \\sim Exp(2)\n\\]\n\nxe &lt;- seq(0, 4, length.out = 1000)\nu &lt;- runif(100000)\nu |&gt; tibble(x = _) |&gt; \nggplot(aes(x=u, y = ..density..)) +\ngeom_histogram() +\ngeom_line(data = tibble(x = xe, y = dexp(xe, rate = 1/2)),\n          mapping = aes(x,y)) +\ntheme_bw()\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nSimulating an Exponential RV\n\n\nCode\nu &lt;- runif(100000)\nx &lt;- -2 * log(1-u)\n\n\n\n\nSimulating an Exponential RV\n\nx |&gt; tibble(x = _) |&gt; \nggplot(aes(x=x, y = ..density..)) +\ngeom_histogram() +\ngeom_line(data = tibble(x = xe, y = dexp(xe, rate = 1/2)),\n          mapping = aes(x,y)) +\ntheme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nExponential RV in R\n\nDescriptionCode\n\n\nThe exponential distribution can be simulated in R using the rexp function with the following arguments:\n\nn: number of values to generate\nrate: how fast would events occur\n\n\n\n\n\nCode\nrexp(1, rate = 1)\n\n\n[1] 1.158092\n\n\n\n\n\n\n\nDiscrete RV Inverse-Transformations\n\nGenerate a random value \\(U\\) that follows a \\(U(0,1)\\)\nUsing the CDF (\\(F(X)\\)), find the smallest integer value \\(k\\) such that:\n\n\\[\nU \\leq F(k)\n\\] 3. \\(X \\leftarrow k\\)\n\n\nPoisson Distribution\n\nxe &lt;- 0:20\nu &lt;- runif(100000)\nu |&gt; tibble(x = _) |&gt; \nggplot(aes(x=x, y = ..density..)) +\ngeom_histogram(bins = 20) +\ngeom_step(data = tibble(x = xe, y = dpois(xe, lambda = 6)),\n          mapping = aes(x,y)) +\ntheme_bw()\n\n\n\n\n\n\n\n\n\n\nPoisson Distribution\n\n\nCode\nfinder &lt;- function(u){\n  x &lt;- 0\n  condition &lt;- TRUE\n  while (condition) {\n    uu &lt;- ppois(x, lambda = 6)\n    condition &lt;- uu &lt;= u\n    if(condition){\n      x &lt;- x + 1\n    }\n  }\n  return(x)\n}\nxx &lt;- sapply(u, finder)  \nxx |&gt; tibble(x = _) |&gt; \nggplot(aes(x=x, y = ..density..)) +\ngeom_histogram(bins = 21) +\ngeom_step(data = tibble(x = xe, y = dpois(xe, lambda = 6)),\n          mapping = aes(x,y)) +\ntheme_bw()\n\n\n\n\n\n\n\n\n\n\n\nExponential RV in R\n\nDescriptionCode\n\n\nThe Poisson distribution can be simulated in R using the rpois function with the following arguments:\n\nn: number of values to generate\nlambda: the average expected event\n\n\n\n\n\nCode\nrpois(1, lambda = 1)\n\n\n[1] 0\n\n\n\n\n\n\n\nNormal Distribution\nObtaining the inverse distribution function of a normal distribution requires the use of numeric algorithms.\nTherefore it is computationally inefficient to use the inverse-transformation algorithm to generate normal random variables. The Box-Muller algorithm was developed to generate 2 standard normal (\\(N(0,1)\\)) random variables from uniform random variables.\n\n\nNormal Distribution\n\\[\ny = \\int^x_{-\\infty}\n\\frac{1}{\\sqrt{2\\pi}} \\exp\\left\\{-\\frac{z^2}{2}\\right\\}dz\n\\]\n\n\nBox-Muller Algorithm\n\nGenerate 2 independent random variables from \\(U(0,1)\\), \\(U_1\\) and \\(U_2\\)\n\\(X_1 = (-2 \\log(U_1))^{1/2}\\cos(2\\pi U_2)\\)\n\\(X_2 = (-2 \\log(U_1))^{1/2}\\sin(2\\pi U_2)\\)\n\nBoth \\(X_1\\) and \\(X_2\\) are independent \\(N(0,1)\\)\n\n\nNormal Distribution R\n\nDescriptionCode\n\n\nThe normal distribution can be simulated in R using the rnorm function with the following arguments:\n\nn: number of values to generate\nmean: the central tendency (peak)\nsd: the variation of the data (width)\n\n\n\n\n\nCode\nrnorm(1, mean = 0, sd = 1)\n\n\n[1] -1.295616"
  },
  {
    "objectID": "mc.html#accept-reject-algorithm",
    "href": "mc.html#accept-reject-algorithm",
    "title": "Monte Carlo Methods",
    "section": "Accept-Reject Algorithm",
    "text": "Accept-Reject Algorithm\nThe Accept-Reject algorithm allows you to generate noncommon random variable by simulating from a common random variable.\n\nAlgorithm Set Up\nLet \\(X\\) be the random variable, that is difficult to generate, you want to generate with a pdf \\(f(x)\\).\nLet \\(Y\\) be an easily generated random variable with a pdf \\(g(y)\\). That follows the same support as \\(f(x)\\)\nLastly, multiply \\(g(y)\\) with a constant \\(c\\) such that \\(f(y)\\leq cg(y)\\).\n\n\nAlgorithm\n\nGenerate \\(Y\\) with a pdf of \\(g(y)\\)\nGenerate \\(U\\) from \\(U(0, cg(y))\\)\nAccept-Reject\nAccept: \\(U\\leq f(y)\\); \\(Y \\rightarrow X\\)\nReject: \\(U&gt;f(y)\\); repeat the algorithm\n\n\n\nModified Algorithm\n\nGenerate \\(Y\\) with a pdf of \\(g(y)\\)\nGenerate \\(U\\) from \\(U(0,1)\\)\nAccept-Reject\nAccept: \\(U\\leq f(y)/(cg(y))\\); \\(Y \\rightarrow X\\)\nReject: \\(U&gt;f(y)/(cg(y))\\); repeat the algorithm\n\n\n\nGamma Random Variable\n\nxe &lt;- seq(0, 20, length.out = 1000)\nxe |&gt; tibble(x = _) |&gt; \nggplot(aes(x=x, y = dgamma(x, shape = 2.3, scale = 1.2))) + \ngeom_line() +\nylab(\"Density\") +\ntheme_bw()\n\n\n\n\n\n\n\n\n\nGamma RV\n\nxe &lt;- seq(0, 20, length.out = 1000)\nx &lt;- rexp(100000)\nx |&gt; tibble(x = _) |&gt; \nggplot(aes(x=x, y = ..density..)) + \ngeom_histogram(aes(color = \"Exponential\")) +\ngeom_line(data = tibble(x = xe, \n                        y = dgamma(x, shape = 2.3, scale = 1.2)), \n          aes(x,y, color = \"Gamma\")) +\nylab(\"Density\") +\ntheme_bw() +\ntheme(legend.position = \"bottom\",\n      legend.title = element_blank())\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nGamma RV\n\nxe &lt;- seq(0, 20, length.out = 1000)\nxe |&gt; tibble(x = _) |&gt; \nggplot(aes(x=x, y = dgamma(x, shape = 2.3, scale = 1.2))) + \ngeom_line(aes(color = \"Gamma\")) +\ngeom_line(data = tibble(x = xe, y = dexp(xe, 1/3)), aes(x,y, color = \"Exponential\")) +\nylab(\"Density\") +\ntheme_bw() +\ntheme(legend.position = \"bottom\",\n      legend.title = element_blank())\n\n\n\n\n\n\n\n\n\n\nAccept-Reject Gamma RV\n\nxe &lt;- seq(0, 20, length.out = 1000)\nxe |&gt; tibble(x = _) |&gt; \nggplot(aes(x=x, y = dgamma(x, shape = 2.3, scale = 1.2))) + \ngeom_line(aes(color = \"Gamma\")) +\ngeom_line(data = tibble(x = xe, y = 1.5*dexp(xe, 1/3)), aes(x,y, color = \"Exponential\")) +\nylab(\"Density\") +\ntheme_bw() +\ntheme(legend.position = \"bottom\",\n      legend.title = element_blank())\n\n\n\n\n\n\n\n\n\n\nAccept-Reject Gamma RV\n\nxe &lt;- seq(0, 20, length.out = 1000)\nxe |&gt; tibble(x = _) |&gt; \nggplot(aes(x=x, y = dgamma(x, shape = 2.3, scale = 1.2))) + \ngeom_line(aes(color = \"Gamma\")) +\ngeom_line(data = tibble(x = xe, y = 3*dexp(xe, 1/3)), aes(x,y, color = \"Exponential\")) +\nylab(\"Density\") +\ntheme_bw() +\ntheme(legend.position = \"bottom\",\n      legend.title = element_blank())\n\n\n\n\n\n\n\n\n\n\nAccept-Reject Gamma RV\n\n\nCode\nx &lt;- c()\nn &lt;- 0\nwhile(n &lt; 10000){\n  e &lt;- rexp(1, 1/2.3)\n  u &lt;- runif(1)\n  f &lt;- dgamma(e, 2.3, 1/1.2)\n  g &lt;- dexp(e, 1/2.3) * 3\n  if (u &lt; (f/g)){\n    x &lt;- c(x, e)\n    n &lt;- length(x)\n  }\n}\n\n\n\n\nGamma RV\n\nx |&gt; tibble(x = _) |&gt; \nggplot(aes(x=x, y = ..density..)) + \ngeom_histogram(aes(color = \"Exponential\")) +\ngeom_line(data = tibble(x = xe, \n                        y = dgamma(x, shape = 2.3, scale = 1.2)), \n          aes(x,y, color = \"Gamma\")) +\nylab(\"Density\") +\ntheme_bw() +\ntheme(legend.position = \"bottom\",\n      legend.title = element_blank())\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nGamma Distribution R\n\nDescriptionCode\n\n\nThe gamma distribution can be simulated in R using the rgamma function with the following arguments:\n\nn: number of values to generate\nshape: describes the shape of distribution (\\(\\alpha\\))\nscale: the spread of the data (\\(\\beta\\))\n\n\n\n\n\nCode\nrgamma(1, shape = 1.2, rate = .5)\n\n\n[1] 1.134533\n\n\n\n\n\n\n\n\nBeta RV in R\n\nDescriptionCode\n\n\nThe beta distribution can be simulated in R using the rbeta function with the following arguments:\n\nn: number of values to generate\nshape1: controls the shape of distribution\nshape2: controls the shape of distribution\n\n\n\n\n\nCode\nrbeta(1, shape1 = 1.2, shape2 = 6.5)\n\n\n[1] 0.2447087\n\n\n\n\n\n\n\nBernoulli RV in R\n\nDescriptionCode\n\n\nThe bernoulli distribution can be simulated in R using the rbinom function with the following arguments:\n\nn: number of values to generate\nsize = 1: will give a bernoulli distribution\nprob: probability of observing 1 (success)\n\n\n\n\n\nCode\nrbinom(1, prob = .2, size = 1)\n\n\n[1] 0\n\n\n\n\n\n\n\nBinomial RV in R\n\nDescriptionCode\n\n\nThe binomial distribution can be simulated in R using the rbinom function with the following arguments:\n\nn: number of values to generate\nsize: how many bernoulli trials to conduct\nprob: probability of observing 1 (success)\n\n\n\n\n\nCode\nrbinom(1, prob = .5, size = 25)\n\n\n[1] 13\n\n\n\n\n\n\n\nNegative Binomial RV in R\n\nDescriptionCode\n\n\nThe negative binomial distribution can be simulated in R using the rnbinom function with the following arguments:\n\nn: number of values to generate\nsize: number of successful trials\nprob: probability of observing 1 (success)\n\n\n\n\n\nCode\nrnbinom(1, prob = .6, size = 5)\n\n\n[1] 1"
  },
  {
    "objectID": "mc.html#transformation-methods",
    "href": "mc.html#transformation-methods",
    "title": "Monte Carlo Methods",
    "section": "Transformation Methods",
    "text": "Transformation Methods\n\n\\(N(0,1)\\)\n\\[\nX \\sim N(\\mu, \\sigma^2)\n\\]\n\\[\nZ = \\frac{X-\\mu}{\\sigma} \\sim N(0,1)\n\\]\n\n\n\\(N(\\mu, \\sigma^2)\\)\n\\[\nZ \\sim N(0,1)\n\\]\n\\[\nX = Z\\sigma + \\mu \\sim N(\\mu, \\sigma^2)\n\\]\n\n\n\\(\\chi^2(1)\\)\n\\[\nZ \\sim N(0,1)\n\\]\n\\[\nZ^2 \\sim \\chi^2(1)\n\\]\n\n\n\\(F(m,n)\\)\n\\[\nU \\sim \\chi^2(m)\n\\]\n\\[\nV \\sim \\chi^2(n)\n\\]\n\\[\nF = \\frac{U/m}{V/n} \\sim F(m,n)\n\\]\n\n\n\\(t(n)\\)\n\\[\nZ \\sim N(0,1)\n\\]\n\\[\nU \\sim \\chi^2(m)\n\\]\n\\[\nT = \\frac{Z}{\\sqrt{U/m}} \\sim t(n)\n\\]\n\n\n\\(Beta(\\alpha, \\beta)\\)\n\\[\nU \\sim Gamma(\\alpha,\\lambda)\n\\]\n\\[\nV \\sim Gamma(\\beta,\\lambda)\n\\]\n\\[\nX = \\frac{U}{U+V} \\sim Beta(\\alpha,\\beta)\n\\]"
  },
  {
    "objectID": "mc.html#monte-carlo-integration",
    "href": "mc.html#monte-carlo-integration",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo Integration",
    "text": "Monte Carlo Integration\nMonte Carlo Integration is a numerical technique to compute a numerical of an integral.\nIt relies on simulating from a know distribution to obtain the expected value of a desired function.\n\nIntegration\nIntegration is commonly used to find the area under a curve.\n\n\nExpectation\nLet \\(X\\) be a continuous random variable:\n\\[\nE(X) = \\int_{X}xf(x)dx\n\\]\n\\[\nE\\{g(X)\\} = \\int_Xg(x)f(x)dx\n\\]\n\n\nStrong Law of Large Numbers\nAs \\(n\\rightarrow \\infty\\) (ie simulate a large number of random variables):\n\\[\n\\bar X_n \\rightarrow E_f(X)\n\\]\nwhere\n\\[\n\\bar X_n \\rightarrow = \\frac{1}{n}\\sum^n_{i=1}X_i\n\\]\n\n\nStrong Law of Large Numbers\n\\[\n\\bar X_n^{(g)} \\rightarrow E_f\\{g(X)\\}\n\\]\nwhere\n\\[\n\\bar X_n^{(g)} \\rightarrow = \\frac{1}{n}\\sum^n_{i=1}g(X_i)\n\\]\n\n\nThe Expected Value of a Normal Distribution\n\\[\nE(X) = \\int^{\\infty}_{-\\infty}\\frac{x}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{-\\frac{(x-\\mu)^2}{\\sigma^2}\\right\\} dx = \\mu\n\\]\n\n\nVariance of a Normal Distribution\n\\[\nVar(X) = E[\\{X-E(X)\\}^2] \\\\= \\int^{\\infty}_{-\\infty}\\frac{\\{x-E(X)\\}^2}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{-\\frac{(x-\\mu)^2}{\\sigma^2}\\right\\} dx = \\sigma^2\n\\]\n\n\nUsing Monte Carlo Integration to obtain expectations\n\nSimulate from a target distribution \\(f\\)\nCalculate the mean for the expected value\n\n\n\nUsing Monte Carlo Integration\n\\[\nX \\sim N(\\mu, \\sigma^2)\n\\]\n\nx &lt;- rnorm(100000, mean = -2, sd = 3)\nmean(x)\n\n[1] -2.019457\n\nvar(x)\n\n[1] 8.929759\n\n\n\n\nGamma Distrbution\n\\[\nX \\sim Gamma(3,4)\n\\]\n\n\nBeta Distribution\n\\[\nX \\sim Beta(2,3)\n\\]\n\n\n\\(\\chi^2(p)\\)\n\\[\nX \\sim \\chi^2(39)\n\\]\n\n\nFinding the Probability\nIntegration is commonly used to determine the probability of observing a certain range of values for a continuous random variable.\n\\[\nP(a &lt; X &lt; b)\n\\]\n\n\nGraphical Setting\n\nx &lt;- seq(-4, 4, length.out = 1000)\ndt_two&lt;-function(x){\n            y &lt;- dnorm(x)\n            y[x&lt; -1 | x&gt;2] &lt;-NA\n            return(y)\n        }\nx |&gt; (\\(.) tibble(x = ., y = dnorm(.)))() |&gt; \n  ggplot(aes(x, y)) +\n    geom_line() +\n    stat_function(fun = dt_two, geom = \"area\", fill = \"green\") + \n    theme_bw()\n\n\n\n\n\n\n\n\n\n\nFinding the Propbabilities of a Random Variable\nFor a given random variable \\(X\\), finding the probability is the same as\n\\[\nE\\{I(a&lt;X&lt;b)\\} = \\int_X I(a&lt;X&lt;b) f(x) dx\n\\]\nwhere \\(I(a&lt;X&lt;b)\\) is the indicator function.\n\n\nIndicator Function\n\\[\nI(a&lt;X&lt;b) = \\left\\{\\begin{array}{cc}\n1 & a&lt;X&lt;b\\\\\n0 & \\mathrm{otherwise}\n\\end{array}\n\\right.\n\\]\n\n\nFinding the Probability\n\\[\n\\begin{align}\nE\\{I(a&lt;X&lt;b)\\} &  = \\int_X I(a&lt;X&lt;b) f(x) dx\\\\\n& = \\int_a^b f(x) dx\\\\\n& = P(a &lt; X &lt; b)\n\\end{align}\n\\]\n\n\nMonte Carlo Probability\n\nSimulate from a target distribution \\(f\\)\nCalculate the mean for \\(I(a&lt;X&lt;b)\\)\n\n\n\nNormal RV Example\nLet \\(X\\sim N(4, 2)\\), find \\(P(3 &lt; X &lt; 6)\\)\n\n\nCode\npnorm(6, 4, sqrt(2)) -  pnorm(3, 4, sqrt(2))\n\n\n[1] 0.6816003\n\n\n\n\nUsing Monte Carlo Methods\n\n\nCode\nx &lt;- rnorm(1000000, 4, sqrt(2))\nmean((x &gt; 3 & x &lt; 6))\n\n\n[1] 0.681875\n\n\n\n\nLogistic RV Example\nLet \\(X\\sim Logistic(3, 5)\\), find \\(P(-1 &lt; X &lt; 5)\\)\n\n\nWeibull RV Example\nLet \\(X\\sim Weibull(1, 1)\\), find \\(P(2 &lt; X &lt; 5.5)\\)\n\n\nF RV Example\nLet \\(X\\sim F(2, 45)\\), find \\(P(1 &lt; X &lt; 3)\\)\n\n\nMonte Carlo Integration\nMonte Carlo Integration can be used to evaluate finite-bounded integrals of the following form:\n\\[\n\\int^b_a g(x) dx\n\\] such that \\(-\\infty &lt;a,b&lt;\\infty\\).\n\n\nMonte Carlo Example Integration\n\\[\n\\int^1_{0} \\{\\cos(50x) - sin(20x)\\}^2dx\n\\]"
  },
  {
    "objectID": "mc.html#monte-carlo-example-integration-1",
    "href": "mc.html#monte-carlo-example-integration-1",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo Example Integration",
    "text": "Monte Carlo Example Integration"
  },
  {
    "objectID": "mc.html#importance-sampling",
    "href": "mc.html#importance-sampling",
    "title": "Monte Carlo Methods",
    "section": "Importance Sampling",
    "text": "Importance Sampling\nImportance sampling is an extension of Monte Carlo integration where it addresses the limitations of large variance of the expected value and the bounds required in integrals.\nThis is done by simulating from a random variable that has an infinite support system.\nLet’s say we are interested in finding the numerical value of the following integral:\n\\[\n\\int_{-\\infty}^\\infty g(x) dx\n\\]\nIf we view the integral as an expectation of an easily simulated random variable, we can compute the numerical value.\nLet \\(X\\) be a random variable \\(f\\), then\n\\[\n\\int_{-\\infty}^\\infty g(x) dx = \\int_{-\\infty}^\\infty \\frac{g(x)}{f(x)} f(x) dx = E\\left\\{\\frac{g(x)}{f(x)}\\right\\}\n\\]\nSince the integral is the expectation of \\(X\\), it can be obtained by taking the mean of the simulated values applied to \\(g(x)/f(x)\\).\n\nExample\n\\[\n\\int_{-\\infty}^{\\infty}  e^{-x^2/2} dx\n\\]\n\n\nExample\n\nx &lt;- rt(1000000, df = 1)\nf2 &lt;- function(x){\n  exp(-x^2/2) / dt(x, 1)\n}\nmean(f2(x))\n\n[1] 2.50614\n\nsqrt(2*pi)\n\n[1] 2.506628\n\n\n\n\nChoosing \\(f(x)\\)\nChoose a value \\(f(x)\\) that follows a shape close enough to \\(g(x)\\) that has the same bounds as the integral."
  }
]